{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Effective Horizon and Effective Planning Window for a given MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EH calculation works as follows:\n",
    "- loads np arrays for the transitions and rewards:\n",
    "    - transitions_array of dimension (_, num_states, num_actions)\n",
    "        - holds ints\n",
    "        - Each cell [i, j] in the array represents the index of the next state that the MDP transitions to when action j is taken in state i. If the next state is a terminal state, the value stored is -1.\n",
    "    - rewards_array of dimension (_, num_states, num_actions) \n",
    "        - holds reward type (set as float32, left to be changed at top of project file)\n",
    "        - Each cell [i, j] stores the reward associated with transitioning from state i to the next state when action j is taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import threading\n",
    "import math\n",
    "\n",
    "# constants and structs\n",
    "class ViResults:\n",
    "    def __init__(self, exploration_qs, exploration_values, optimal_qs, optimal_values, worst_qs, worst_values, visitable_states):\n",
    "        self.exploration_qs = exploration_qs\n",
    "        self.exploration_values = exploration_values\n",
    "        self.optimal_qs = optimal_qs\n",
    "        self.optimal_values = optimal_values\n",
    "        self.worst_qs = worst_qs\n",
    "        self.worst_values = worst_values\n",
    "        self.visitable_states = visitable_states\n",
    "\n",
    "class EHResults:\n",
    "    def __init__(self, ks, ms, vars, gaps, effective_horizon):\n",
    "        self.ks = ks\n",
    "        self.ms = ms\n",
    "        self.vars = vars\n",
    "        self.gaps = gaps\n",
    "        self.effective_horizon = effective_horizon\n",
    "\n",
    "REWARD_PRECISION = 1e-4\n",
    "\n",
    "# setup lock and findmax\n",
    "lock = threading.Lock()\n",
    "\n",
    "def findmax(arr):\n",
    "    max_val = np.max(arr)\n",
    "    max_index = np.argmax(arr)\n",
    "    return max_val, max_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pipeline: functions to get bounds and their helpers ###\n",
    "\n",
    "## OUTSTANDING TODO: ##\n",
    "    # substitute env.P with transition table generating code loaded from consolidated.npz file we can load in per mdp from dataset\n",
    "    # ensure all variables are being updated in the proper scope (ie. threading modifications are passed through to the variables at least when the threading is over)\n",
    "    # clean up documentation\n",
    "\n",
    "# multithreading value_iteration helper: process a timestep of vi visitable states to get output vals\n",
    "def process_timestep_thread(ts, transitions, rewards, state, num_actions, horizon, exploration_values, exploration_qs, optimal_qs, optimal_values, worst_qs, worst_values,exploration_policy=None):\n",
    "    for action in num_actions:\n",
    "        obs = transitions[state][action]\n",
    "        reward = rewards[state][action]\n",
    "        # _, obs, reward, _ = env.P[state][action]\n",
    "        if ts < horizon:\n",
    "            exploration_qs[ts, state, action] = reward + exploration_values[ts+1, obs]\n",
    "            optimal_qs[ts, state, action] = reward + optimal_values[ts+1, obs]\n",
    "            worst_qs[ts, state, action] = reward + worst_values[ts+1, obs]\n",
    "        else:\n",
    "            exploration_qs[ts, state, action] = reward\n",
    "            optimal_qs[ts, state, action] = reward\n",
    "            worst_qs[ts, state, action] = reward\n",
    "    \n",
    "    optimal_value = max(optimal_qs[ts, state, :])\n",
    "    worst_value = min(worst_qs[ts, state, :])\n",
    "    if exploration_policy == None:\n",
    "        exploration_value =  sum(exploration_qs[ts, state, :]) / num_actions\n",
    "    else:\n",
    "        exploration_value=0\n",
    "        for action in range(num_actions):\n",
    "            exploration_value += exploration_qs[ts, state, action] * exploration_policy[ts, state, action]\n",
    "    \n",
    "    optimal_values[ts, state] = optimal_value\n",
    "    worst_values[ts, state] = worst_value\n",
    "\n",
    "    # verification: no nans and no float errs\n",
    "    # ensure exploration val is bw worst and optimal vals (avoid floating pt errors)\n",
    "    exploration_values[ts, state] = min(optimal_value, max(worst_value, exploration_value))\n",
    "    assert(not np.isnan(exploration_values[ts, state]))\n",
    "    assert(not np.isnan(optimal_values[ts, state]))\n",
    "    assert(not np.isnan(worst_values[ts, state]))\n",
    "\n",
    "\n",
    "# value iteration: calculate exploration_qs, exploration_values, optimal_qs, optimal_values, worst_qs, worst_values, visitable_states, returns in single results structure in this order\n",
    "def value_iteration(transitions, rewards, horizon, exploration_policy=None):\n",
    "    # get constants from environment\n",
    "    num_states = len(transitions) #env.observation_space.n\n",
    "    num_actions = len(transitions[0]) #env.action_space.n\n",
    "\n",
    "    # get list of visitable states where each item is a set of visitable states at a given timestep (index)\n",
    "    visitable_states = []\n",
    "    current_visitable_states = set()\n",
    "    current_visitable_states.add(0)\n",
    "    for _ in tqdm(range(horizon)):\n",
    "        next_visitable_states = set()\n",
    "        for state in current_visitable_states:\n",
    "            for action in range(num_actions):\n",
    "                next_state = transitions[state][action]\n",
    "                # _, next_state, _, _ = env.P[state][action]\n",
    "                next_visitable_states.add(next_state)\n",
    "        visitable_states.append(next_visitable_states.union(current_visitable_states))\n",
    "        current_visitable_states = next_visitable_states\n",
    "    \n",
    "    # initialize outputs\n",
    "    exploration_qs = np.full((horizon, num_states, num_actions), np.nan)\n",
    "    exploration_values = np.full((horizon, num_states), np.nan)\n",
    "    optimal_qs = np.full((horizon, num_states, num_actions), np.nan)\n",
    "    optimal_values = np.full((horizon, num_states), np.nan)\n",
    "    worst_qs = np.full((horizon, num_states, num_actions), np.nan)\n",
    "    worst_values = np.full((horizon, num_states), np.nan)\n",
    "\n",
    "    # get output vals for vi\n",
    "    for timestep in tqdm(range(horizon)):\n",
    "        # thread across different states in a given timestep, so no overwriting vals in data structures\n",
    "        threads = []\n",
    "        for state in visitable_states[timestep]:\n",
    "            thread = threading.Thread(target=process_timestep_thread(timestep, transitions, rewards, state, num_actions, horizon, exploration_values, exploration_qs, optimal_qs, optimal_values, worst_qs, worst_values,exploration_policy))\n",
    "            threads.append(thread)\n",
    "        # run the threads\n",
    "        for thread in threads:\n",
    "            thread.start()\n",
    "        # merge the results of the threads\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "\n",
    "    # define results struct and return results\n",
    "    results = ViResults(exploration_qs, exploration_values, optimal_qs, optimal_values, worst_qs, worst_values, visitable_states)\n",
    "    return results\n",
    "\n",
    "\n",
    "# compute variance bounds: finds a bound on the variance of the qs based on the best and worst qs for each state at a timestep\n",
    "def compute_variance_thread(var_bounds, ts, state, num_actions, vi):\n",
    "    for action in range(num_actions):\n",
    "        q = vi.exploration_qs[ts, state, action]\n",
    "        worst_q = vi.worst_qs[ts,state,action]\n",
    "        optimal_q = vi.optimal_qs[ts, state, action]\n",
    "        var_bound = (q - worst_q) * (optimal_q - worst_q)\n",
    "        var_bounds[ts, state, action] = var_bound\n",
    "\n",
    "\n",
    "# k iteration thread: checks k validity and adds to seeable next states in following timestep\n",
    "def k_working_thread(k_works, ts, state, states_can_be_visited, num_actions, current_qs, vi, var_bounds, state_gaps, state_vars, horizon,k, state_ms, transitions, rewards):\n",
    "    if states_can_be_visited[ts, state]:\n",
    "        # init vals\n",
    "        max_q = -np.inf\n",
    "        max_suboptimal_q = -np.inf\n",
    "        max_var = 0\n",
    "        # iterate over actions to get actual vals\n",
    "        for action in range(num_actions):\n",
    "            q = current_qs[ts, state, action]\n",
    "            max_q = max(max_q, q)\n",
    "            if vi.optimal_qs[ts,state,action] < vi.optimal_values[ts, state] - REWARD_PRECISION:\n",
    "                max_suboptimal_q = max(max_suboptimal_q, q)\n",
    "            max_var = max(max_var, var_bounds[ts, state, action])\n",
    "            # check for k fail condition\n",
    "            if max_q == max_suboptimal_q:\n",
    "                with lock:\n",
    "                    k_works = False\n",
    "            # otherwise get the state m value\n",
    "            else:\n",
    "                gap = max_q - max_suboptimal_q\n",
    "                state_gaps[ts, state] = gap\n",
    "                state_vars[ts, state] = max_var\n",
    "                m = math.ceil(16 * max_var / (gap**2) * math.log(2 * horizon * (num_actions**k)))\n",
    "                state_ms[ts, state] = max(1,m)\n",
    "            # iterate through actions to find next visitable states\n",
    "                for action in range(num_actions):\n",
    "                    if current_qs[ts, state, action] > max_suboptimal_q:\n",
    "                        next_state = transitions[state][action]\n",
    "                        # _, next_state, _, _ = env.P[state][action]\n",
    "                        if ts < horizon:\n",
    "                            states_can_be_visited[ts+1, next_state] = True\n",
    "\n",
    "\n",
    "# bellman backup thread: update the current qs and variance bounds for the run\n",
    "def bellman_backup_thread(ts, num_actions, transitions, rewards, state, current_qs, var_bounds):\n",
    "    for action in range(num_actions):\n",
    "        next_state = transitions[state][action]\n",
    "        reward = rewards[state][action]\n",
    "        #_, next_state, reward, _ = env.P[state][action]\n",
    "        max_next_q = -np.inf\n",
    "        max_next_var_bound = 0\n",
    "        for action in range(num_actions):\n",
    "            next_q = current_qs[ts+1, next_state, action]\n",
    "            max_next_q = max(max_next_q, next_q)\n",
    "            next_var_bound = var_bounds[ts+1, next_state, action]\n",
    "            max_next_var_bound = max(max_next_var_bound, next_var_bound)\n",
    "        current_qs[ts, state, action] = reward + max_next_q\n",
    "        var_bounds[ts, state, action] = max_next_var_bound\n",
    "\n",
    "\n",
    "# calculate EH bound using GORP bounds\n",
    "def get_EH_bound(transitions, rewards, horizon, exploration_policy=None):\n",
    "    num_states = len(transitions)#env.observation_space.n\n",
    "    num_actions = len(transitions[0])#env.action_space.n\n",
    "    # Perform value iteration\n",
    "    vi = value_iteration(transitions, rewards, horizon, exploration_policy)\n",
    "    \n",
    "    # initialize and compute variance bounds\n",
    "    var_bounds = np.full((horizon, num_states, num_actions), np.nan)\n",
    "    for timestep in tqdm(range(horizon)):\n",
    "        # thread across different states in a given timestep, so no overwriting vals in data structures\n",
    "        threads = []\n",
    "        for state in vi.visitable_states[timestep]:\n",
    "            thread = threading.Thread(target=compute_variance_thread(var_bounds, timestep, state, num_actions, vi))\n",
    "            threads.append(thread)\n",
    "        # run the threads\n",
    "        for thread in threads:\n",
    "            thread.start()\n",
    "        # merge the results of the threads\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "\n",
    "    # initialize q values, starting k, and EH results structure\n",
    "    current_qs = vi.exploration_qs\n",
    "    results = EHResults([],[],[],[],horizon)\n",
    "    k = 1\n",
    "\n",
    "    # find best working k\n",
    "    while k < results.effective_horizon:\n",
    "        # initialize results objects and iteration\n",
    "        k_works = True\n",
    "        state_ms = np.zeros(horizon, num_states)\n",
    "        state_vars = np.zeros(horizon, num_states)\n",
    "        state_gaps = np.zeros(horizon, num_states)\n",
    "        states_can_be_visited = np.full((horizon, num_states), False)\n",
    "        states_can_be_visited[0,0] = True\n",
    "        \n",
    "        for timestep in tqdm(range(horizon)):\n",
    "            # thread across different states in a given timestep, so no overwriting vals in data structures\n",
    "            threads = []\n",
    "            for state in vi.visitable_states[timestep]:\n",
    "                thread = threading.Thread(target=k_working_thread(k_works, timestep, state, states_can_be_visited, num_actions, current_qs, vi, var_bounds, state_gaps, state_vars, horizon, k, state_ms, transitions, rewards))\n",
    "                threads.append(thread)\n",
    "            # run the threads\n",
    "            for thread in threads:\n",
    "                thread.start()\n",
    "            # merge the results of the threads\n",
    "            for thread in threads:\n",
    "                thread.join()\n",
    "        if not k_works:\n",
    "            break\n",
    "    \n",
    "    # if k works flag never triggered, then k is horizon and we update result vals accordingly\n",
    "    if k_works:\n",
    "        results.ks.append(k)\n",
    "        highest_m, timestep_state = findmax(state_ms)\n",
    "        timestep, state = timestep_state\n",
    "        results.ms.append(highest_m)\n",
    "        # log of highest_m with base num_actions\n",
    "        H_k = k + math.log(highest_m, num_actions)\n",
    "        print(f\"H_{k} = {H_k}\")\n",
    "        results.gaps.append(state_gaps[timestep, state])\n",
    "        results.vars.append(state_vars[timestep, state])\n",
    "        results.effective_horizon = min(results.effective_horizon, H_k)\n",
    "    \n",
    "    # run a bellman backup\n",
    "    for timestep in tqdm(range(horizon-1)):\n",
    "        # thread across different states in a given timestep, so no overwriting vals in data structures\n",
    "            threads = []\n",
    "            for state in vi.visitable_states[timestep]:\n",
    "                thread = threading.Thread(target=bellman_backup_thread(timestep, num_actions, transitions, rewards, state, current_qs, var_bounds))\n",
    "                threads.append(thread)\n",
    "            # run the threads\n",
    "            for thread in threads:\n",
    "                thread.start()\n",
    "            # merge the results of the threads\n",
    "            for thread in threads:\n",
    "                thread.join()\n",
    "                \n",
    "    k += 1\n",
    "    return results\n",
    "    \n",
    "\n",
    "# k_working_epw thread: checks if given state works for k\n",
    "def k_working_epw_thread(k_works, states_can_be_visited, ts, state, num_actions, current_qs, vi, transitions, rewards, horizon):\n",
    "    if states_can_be_visited[ts, state]:\n",
    "        # init and find max q\n",
    "        max_q = -np.inf\n",
    "        for action in range(num_actions):\n",
    "            max_q = max(max_q, current_qs[ts, state, action])\n",
    "        for action in range(num_actions):\n",
    "            # check if possible to take this action\n",
    "            if current_qs[ts, state, action] >= max_q:\n",
    "                if vi.optimal_qs[ts, state, action] < vi.optimal_values[ts,state] - REWARD_PRECISION:\n",
    "                    k_works = False\n",
    "                next_state = transitions[state][action]\n",
    "                #_, next_state, _, _ = env.P[state][action]\n",
    "                if ts < horizon:\n",
    "                    states_can_be_visited[ts+1, next_state] = True\n",
    "\n",
    "\n",
    "# epw_bellman_backup, modified bellman backup for epw\n",
    "def epw_bellman_backup_thread(ts, num_actions, state, transitions, rewards, current_qs):\n",
    "    for action in range(num_actions):\n",
    "        next_state = transitions[state][action]\n",
    "        reward = rewards[state][action]\n",
    "        #_, next_state, reward, _ = env.P[state][action]\n",
    "        max_next_q = -np.inf\n",
    "        for action in range(num_actions):\n",
    "            next_q = current_qs[ts+1, next_state, action]\n",
    "            max_next_q = max(max_next_q, next_q)\n",
    "        current_qs[ts, state, action] = reward + max_next_q\n",
    "\n",
    "\n",
    "# calculate EPW\n",
    "def get_EPW(transitions, rewards, horizon, exploration_policy=None, start_with_rewards=True):\n",
    "    # calculate min k for the gym environment and return it\n",
    "    num_states = len(transitions)#env.observation_space.n\n",
    "    num_actions = len(transitions[0])#env.action_space.n\n",
    "    # Perform value iteration\n",
    "    vi = value_iteration(transitions, rewards, horizon, exploration_policy)\n",
    "\n",
    "    # init current_qs\n",
    "    if start_with_rewards:\n",
    "        current_qs = np.full((horizon, num_states, num_actions), np.nan)\n",
    "        for timestep in tqdm(range(horizon)):\n",
    "            for state in vi.visitable_states[timestep]:\n",
    "                for action in range(num_actions):\n",
    "                    current_qs[timestep, state, action] = rewards[state][action]\n",
    "                    #_, _, current_qs[timestep, state, action], _ = env.P[state][action]\n",
    "    else:\n",
    "        current_qs = vi.exploration_qs\n",
    "\n",
    "    #\n",
    "    k = 1\n",
    "    while True:\n",
    "        # check if this k value works\n",
    "        k_works = True\n",
    "        states_can_be_visited = np.full((horizon, num_states), False)\n",
    "        states_can_be_visited[0,0] = True\n",
    "        for timestep in tqdm(range(horizon)):\n",
    "            # thread across different states in a given timestep, so no overwriting vals in data structures\n",
    "            threads = []\n",
    "            for state in vi.visitable_states[timestep]:\n",
    "                thread = threading.Thread(target=k_working_epw_thread(k_works, states_can_be_visited, timestep, state, num_actions, current_qs, vi, transitions, rewards, horizon))\n",
    "                threads.append(thread)\n",
    "            # run the threads\n",
    "            for thread in threads:\n",
    "                thread.start()\n",
    "            # merge the results of the threads\n",
    "            for thread in threads:\n",
    "                thread.join()\n",
    "        if not k_works:\n",
    "            break\n",
    "    \n",
    "    if k_works:\n",
    "        return k\n",
    "        \n",
    "    # otherwise run bellman backup and up k\n",
    "    for timestep in tqdm(range(horizon)):\n",
    "        # thread across different states in a given timestep, so no overwriting vals in data structures\n",
    "        threads = []\n",
    "        for state in vi.visitable_states[timestep]:\n",
    "            thread = threading.Thread(target=epw_bellman_backup_thread())\n",
    "            threads.append(thread)\n",
    "        # run the threads\n",
    "        for thread in threads:\n",
    "            thread.start()\n",
    "        # merge the results of the threads\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratchwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figuring out .npz structure of transitions and rewards. From documentation:\n",
    "\n",
    "`consolidated.npz`: the tabular representation of the MDP with consolidated states. States are consolidated if any sequence of actions from them will always lead to the same sequence of observations and rewards. The file format is a NumPy NPZ archive with two arrays, `transitions` and `rewards`, each of shape `(num_states, num_actions)`. `transitions[state, action]` gives the index of the next state reached by taking action `action` in state `state`, or -1 if the next state is terminal; `rewards[state, action]` gives the reward accompanying the aforementioned transition. The intial state has index 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "pong_table = np.load('/Users/laurenc/Documents/GitHub/282_expansion/data/bridge_dataset/mdps/pong_20_fs30/consolidated.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the npz file: ['rewards', 'transitions']\n",
      "\n",
      "Array: rewards\n",
      "Shape: (254, 6)\n",
      "Data type: float32\n",
      "First few rows:\n",
      "[[ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [-1. -1. -1. -1. -1. -1.]]\n",
      "\n",
      "Array: transitions\n",
      "Shape: (254, 6)\n",
      "Data type: int64\n",
      "First few rows:\n",
      "[[123 123  70  28  70  28]\n",
      " [ 66  66  66 242  66 242]\n",
      " [138 138 138 114 138 114]\n",
      " [ 41  41  41  20  41  20]\n",
      " [209 209 209 108 209 108]]\n"
     ]
    }
   ],
   "source": [
    "# Print the keys (names of the arrays saved in the file)\n",
    "print(\"Keys in the npz file:\", list(pong_table.keys()))\n",
    "\n",
    "# Print the structure and the first few rows of each array\n",
    "for key in pong_table.keys():\n",
    "    print(\"\\nArray:\", key)\n",
    "    print(\"Shape:\", pong_table[key].shape)\n",
    "    print(\"Data type:\", pong_table[key].dtype)\n",
    "    print(\"First few rows:\")\n",
    "    print(pong_table[key][:5])  # Print the first 5 rows of the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
