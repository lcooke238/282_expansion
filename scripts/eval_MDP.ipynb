{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Effective Horizon and Effective Planning Window for a given MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EH calculation works as follows:\n",
    "- loads np arrays for the transitions and rewards:\n",
    "    - transitions_array of dimension (_, num_states, num_actions)\n",
    "        - holds ints\n",
    "        - Each cell [i, j] in the array represents the index of the next state that the MDP transitions to when action j is taken in state i. If the next state is a terminal state, the value stored is -1.\n",
    "    - rewards_array of dimension (_, num_states, num_actions) \n",
    "        - holds reward type (set as float32, left to be changed at top of project file)\n",
    "        - Each cell [i, j] stores the reward associated with transitioning from state i to the next state when action j is taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import threading\n",
    "import math\n",
    "\n",
    "# constants and structs\n",
    "class ViResults:\n",
    "    def __init__(self, exploration_qs, exploration_values, optimal_qs, optimal_values, worst_qs, worst_values, visitable_states):\n",
    "        self.exploration_qs = exploration_qs\n",
    "        self.exploration_values = exploration_values\n",
    "        self.optimal_qs = optimal_qs\n",
    "        self.optimal_values = optimal_values\n",
    "        self.worst_qs = worst_qs\n",
    "        self.worst_values = worst_values\n",
    "        self.visitable_states = visitable_states\n",
    "\n",
    "class EHResults:\n",
    "    def __init__(self, ks, ms, vars, gaps, effective_horizon):\n",
    "        self.ks = ks\n",
    "        self.ms = ms\n",
    "        self.vars = vars\n",
    "        self.gaps = gaps\n",
    "        self.effective_horizon = effective_horizon\n",
    "\n",
    "REWARD_PRECISION = 1e-4\n",
    "\n",
    "# setup lock and other julia operations\n",
    "lock = threading.Lock()\n",
    "\n",
    "def findmax(arr):\n",
    "    max_val = np.max(arr)\n",
    "    max_index = np.argmax(arr)\n",
    "    return max_val, max_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pipeline: functions to get bounds and their helpers ###\n",
    "\n",
    "## OUTSTANDING TODO: ##\n",
    "    # substitute env.P with transition table generating code loaded from consolidated.npz file we can load in per mdp from dataset\n",
    "    # ensure all variables are being updated in the proper scope (ie. threading modifications are passed through to the variables at least when the threading is over)\n",
    "    # clean up documentation\n",
    "\n",
    "# multithreading value_iteration helper: process a timestep of vi visitable states to get output vals\n",
    "def process_timestep_thread(ts, env, state, num_actions, horizon, exploration_values, exploration_qs, optimal_qs, optimal_values, worst_qs, worst_values,exploration_policy=None):\n",
    "    for action in num_actions:\n",
    "        _, obs, reward, _ = env.P[state][action]\n",
    "        if ts < horizon:\n",
    "            exploration_qs[ts, state, action] = reward + exploration_values[ts+1, obs]\n",
    "            optimal_qs[ts, state, action] = reward + optimal_values[ts+1, obs]\n",
    "            worst_qs[ts, state, action] = reward + worst_values[ts+1, obs]\n",
    "        else:\n",
    "            exploration_qs[ts, state, action] = reward\n",
    "            optimal_qs[ts, state, action] = reward\n",
    "            worst_qs[ts, state, action] = reward\n",
    "    \n",
    "    optimal_value = max(optimal_qs[ts, state, :])\n",
    "    worst_value = min(worst_qs[ts, state, :])\n",
    "    if exploration_policy == None:\n",
    "        exploration_value =  sum(exploration_qs[ts, state, :]) / num_actions\n",
    "    else:\n",
    "        exploration_value=0\n",
    "        for action in range(num_actions):\n",
    "            exploration_value += exploration_qs[ts, state, action] * exploration_policy[ts, state, action]\n",
    "    \n",
    "    optimal_values[ts, state] = optimal_value\n",
    "    worst_values[ts, state] = worst_value\n",
    "\n",
    "    # verification: no nans and no float errs\n",
    "    # ensure exploration val is bw worst and optimal vals (avoid floating pt errors)\n",
    "    exploration_values[ts, state] = min(optimal_value, max(worst_value, exploration_value))\n",
    "    assert(not np.isnan(exploration_values[ts, state]))\n",
    "    assert(not np.isnan(optimal_values[ts, state]))\n",
    "    assert(not np.isnan(worst_values[ts, state]))\n",
    "\n",
    "\n",
    "# value iteration: calculate exploration_qs, exploration_values, optimal_qs, optimal_values, worst_qs, worst_values, visitable_states, returns in single results structure in this order\n",
    "def value_iteration(env, horizon, exploration_policy=None):\n",
    "    # get constants from environment\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    # get list of visitable states where each item is a set of visitable states at a given timestep (index)\n",
    "    visitable_states = []\n",
    "    current_visitable_states = set()\n",
    "    current_visitable_states.add(0)\n",
    "    for _ in tqdm(range(horizon)):\n",
    "        next_visitable_states = set()\n",
    "        for state in current_visitable_states:\n",
    "            for action in range(num_actions):\n",
    "                _, next_state, _, _ = env.P[state][action]\n",
    "                next_visitable_states.add(next_state)\n",
    "        visitable_states.append(next_visitable_states.union(current_visitable_states))\n",
    "        current_visitable_states = next_visitable_states\n",
    "    \n",
    "    # initialize outputs\n",
    "    exploration_qs = np.full((horizon, num_states, num_actions), np.nan)\n",
    "    exploration_values = np.full((horizon, num_states), np.nan)\n",
    "    optimal_qs = np.full((horizon, num_states, num_actions), np.nan)\n",
    "    optimal_values = np.full((horizon, num_states), np.nan)\n",
    "    worst_qs = np.full((horizon, num_states, num_actions), np.nan)\n",
    "    worst_values = np.full((horizon, num_states), np.nan)\n",
    "\n",
    "    # get output vals for vi\n",
    "    for timestep in tqdm(range(horizon)):\n",
    "        # thread across different states in a given timestep, so no overwriting vals in data structures\n",
    "        threads = []\n",
    "        for state in visitable_states[timestep]:\n",
    "            thread = threading.Thread(target=process_timestep_thread(timestep, env, state, num_actions, horizon, exploration_values, exploration_qs, optimal_qs, optimal_values, worst_qs, worst_values,exploration_policy))\n",
    "            threads.append(thread)\n",
    "        # run the threads\n",
    "        for thread in threads:\n",
    "            thread.start()\n",
    "        # merge the results of the threads\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "\n",
    "    # define results struct and return results\n",
    "    results = ViResults(exploration_qs, exploration_values, optimal_qs, optimal_values, worst_qs, worst_values, visitable_states)\n",
    "    return results\n",
    "\n",
    "\n",
    "# compute variance bounds: finds a bound on the variance of the qs based on the best and worst qs for each state at a timestep\n",
    "def compute_variance_thread(var_bounds, ts, state, num_actions, vi):\n",
    "    for action in range(num_actions):\n",
    "        q = vi.exploration_qs[ts, state, action]\n",
    "        worst_q = vi.worst_qs[ts,state,action]\n",
    "        optimal_q = vi.optimal_qs[ts, state, action]\n",
    "        var_bound = (q - worst_q) * (optimal_q - worst_q)\n",
    "        var_bounds[ts, state, action] = var_bound\n",
    "\n",
    "\n",
    "# k iteration thread: checks k validity and adds to seeable next states in following timestep\n",
    "def k_working_thread(k_works, ts, state, states_can_be_visited, num_actions, current_qs, vi, var_bounds, state_gaps, state_vars, horizon,k, state_ms, env):\n",
    "    if states_can_be_visited[ts, state]:\n",
    "        # init vals\n",
    "        max_q = -np.inf\n",
    "        max_suboptimal_q = -np.inf\n",
    "        max_var = 0\n",
    "        # iterate over actions to get actual vals\n",
    "        for action in range(num_actions):\n",
    "            q = current_qs[ts, state, action]\n",
    "            max_q = max(max_q, q)\n",
    "            if vi.optimal_qs[ts,state,action] < vi.optimal_values[ts, state] - REWARD_PRECISION:\n",
    "                max_suboptimal_q = max(max_suboptimal_q, q)\n",
    "            max_var = max(max_var, var_bounds[ts, state, action])\n",
    "            # check for k fail condition\n",
    "            if max_q == max_suboptimal_q:\n",
    "                with lock:\n",
    "                    k_works = False\n",
    "            # otherwise get the state m value\n",
    "            else:\n",
    "                gap = max_q - max_suboptimal_q\n",
    "                state_gaps[ts, state] = gap\n",
    "                state_vars[ts, state] = max_var\n",
    "                m = math.ceil(16 * max_var / (gap**2) * math.log(2 * horizon * (num_actions**k)))\n",
    "                state_ms[ts, state] = max(1,m)\n",
    "            # iterate through actions to find next visitable states\n",
    "                for action in range(num_actions):\n",
    "                    if current_qs[ts, state, action] > max_suboptimal_q:\n",
    "                        _, next_state, _, _ = env.P[state][action]\n",
    "                        if ts < horizon:\n",
    "                            states_can_be_visited[ts+1, next_state] = True\n",
    "\n",
    "\n",
    "# bellman backup thread: update the current qs and variance bounds for the run\n",
    "def bellman_backup_thread(ts, num_actions, env, state, current_qs, var_bounds):\n",
    "    for action in range(num_actions):\n",
    "        _, next_state, reward, _ = env.P[state][action]\n",
    "        max_next_q = -np.inf\n",
    "        max_next_var_bound = 0\n",
    "        for action in range(num_actions):\n",
    "            next_q = current_qs[ts+1, next_state, action]\n",
    "            max_next_q = max(max_next_q, next_q)\n",
    "            next_var_bound = var_bounds[ts+1, next_state, action]\n",
    "            max_next_var_bound = max(max_next_var_bound, next_var_bound)\n",
    "        current_qs[ts, state, action] = reward + max_next_q\n",
    "        var_bounds[ts, state, action] = max_next_var_bound\n",
    "\n",
    "\n",
    "# calculate EH bound using GORP bounds\n",
    "def get_EH_bound(env, horizon, exploration_policy=None):\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "    # Perform value iteration\n",
    "    vi = value_iteration(env, horizon, exploration_policy)\n",
    "    \n",
    "    # initialize and compute variance bounds\n",
    "    var_bounds = np.full((horizon, num_states, num_actions), np.nan)\n",
    "    for timestep in tqdm(range(horizon)):\n",
    "        # thread across different states in a given timestep, so no overwriting vals in data structures\n",
    "        threads = []\n",
    "        for state in vi.visitable_states[timestep]:\n",
    "            thread = threading.Thread(target=compute_variance_thread(var_bounds, timestep, state, num_actions, vi))\n",
    "            threads.append(thread)\n",
    "        # run the threads\n",
    "        for thread in threads:\n",
    "            thread.start()\n",
    "        # merge the results of the threads\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "\n",
    "    # initialize q values, starting k, and EH results structure\n",
    "    current_qs = vi.exploration_qs\n",
    "    results = EHResults([],[],[],[],horizon)\n",
    "    k = 1\n",
    "\n",
    "    # find best working k\n",
    "    while k < results.effective_horizon:\n",
    "        # initialize results objects and iteration\n",
    "        k_works = True\n",
    "        state_ms = np.zeros(horizon, num_states)\n",
    "        state_vars = np.zeros(horizon, num_states)\n",
    "        state_gaps = np.zeros(horizon, num_states)\n",
    "        states_can_be_visited = np.full((horizon, num_states), False)\n",
    "        states_can_be_visited[0,0] = True\n",
    "        \n",
    "        for timestep in tqdm(range(horizon)):\n",
    "            # thread across different states in a given timestep, so no overwriting vals in data structures\n",
    "            threads = []\n",
    "            for state in vi.visitable_states[timestep]:\n",
    "                thread = threading.Thread(target=k_working_thread(k_works, timestep, state, states_can_be_visited, num_actions, current_qs, vi, var_bounds, state_gaps, state_vars, horizon, k, state_ms, env))\n",
    "                threads.append(thread)\n",
    "            # run the threads\n",
    "            for thread in threads:\n",
    "                thread.start()\n",
    "            # merge the results of the threads\n",
    "            for thread in threads:\n",
    "                thread.join()\n",
    "        if not k_works:\n",
    "            break\n",
    "    \n",
    "    # if k works flag never triggered, then k is horizon and we update result vals accordingly\n",
    "    if k_works:\n",
    "        results.ks.append(k)\n",
    "        highest_m, timestep_state = findmax(state_ms)\n",
    "        timestep, state = timestep_state\n",
    "        results.ms.append(highest_m)\n",
    "        # log of highest_m with base num_actions\n",
    "        H_k = k + math.log(highest_m, num_actions)\n",
    "        print(f\"H_{k} = {H_k}\")\n",
    "        results.gaps.append(state_gaps[timestep, state])\n",
    "        results.vars.append(state_vars[timestep, state])\n",
    "        results.effective_horizon = min(results.effective_horizon, H_k)\n",
    "    \n",
    "    # run a bellman backup\n",
    "    for timestep in tqdm(range(horizon-1)):\n",
    "        # thread across different states in a given timestep, so no overwriting vals in data structures\n",
    "            threads = []\n",
    "            for state in vi.visitable_states[timestep]:\n",
    "                thread = threading.Thread(target=bellman_backup_thread(timestep, num_actions, env, state, current_qs, var_bounds))\n",
    "                threads.append(thread)\n",
    "            # run the threads\n",
    "            for thread in threads:\n",
    "                thread.start()\n",
    "            # merge the results of the threads\n",
    "            for thread in threads:\n",
    "                thread.join()\n",
    "                \n",
    "    k += 1\n",
    "    return results\n",
    "    \n",
    "\n",
    "# k_working_epw thread: checks if given state works for k\n",
    "def k_working_epw_thread(k_works, states_can_be_visited, ts, state, num_actions, current_qs, vi, env, horizon):\n",
    "    if states_can_be_visited[ts, state]:\n",
    "        # init and find max q\n",
    "        max_q = -np.inf\n",
    "        for action in range(num_actions):\n",
    "            max_q = max(max_q, current_qs[ts, state, action])\n",
    "        for action in range(num_actions):\n",
    "            # check if possible to take this action\n",
    "            if current_qs[ts, state, action] >= max_q:\n",
    "                if vi.optimal_qs[ts, state, action] < vi.optimal_values[ts,state] - REWARD_PRECISION:\n",
    "                    k_works = False\n",
    "                _, next_state, _, _ = env.P[state][action]\n",
    "                if ts < horizon:\n",
    "                    states_can_be_visited[ts+1, next_state] = True\n",
    "\n",
    "\n",
    "# epw_bellman_backup, modified bellman backup for epw\n",
    "def epw_bellman_backup_thread(ts, num_actions, state, env, current_qs):\n",
    "    for action in range(num_actions):\n",
    "        _, next_state, reward, _ = env.P[state][action]\n",
    "        max_next_q = -np.inf\n",
    "        for action in range(num_actions):\n",
    "            next_q = current_qs[ts+1, next_state, action]\n",
    "            max_next_q = max(max_next_q, next_q)\n",
    "        current_qs[ts, state, action] = reward + max_next_q\n",
    "\n",
    "\n",
    "# calculate EPW\n",
    "def get_EPW(env, horizon, exploration_policy=None, start_with_rewards=True):\n",
    "    # calculate min k for the gym environment and return it\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "    # Perform value iteration\n",
    "    vi = value_iteration(env, horizon, exploration_policy)\n",
    "\n",
    "    # init current_qs\n",
    "    if start_with_rewards:\n",
    "        current_qs = np.full((horizon, num_states, num_actions), np.nan)\n",
    "        for timestep in tqdm(range(horizon)):\n",
    "            for state in vi.visitable_states[timestep]:\n",
    "                for action in range(num_actions):\n",
    "                    _, _, current_qs[timestep, state, action], _ = env.P[state][action]\n",
    "    else:\n",
    "        current_qs = vi.exploration_qs\n",
    "\n",
    "    #\n",
    "    k = 1\n",
    "    while True:\n",
    "        # check if this k value works\n",
    "        k_works = True\n",
    "        states_can_be_visited = np.full((horizon, num_states), False)\n",
    "        states_can_be_visited[0,0] = True\n",
    "        for timestep in tqdm(range(horizon)):\n",
    "            # thread across different states in a given timestep, so no overwriting vals in data structures\n",
    "            threads = []\n",
    "            for state in vi.visitable_states[timestep]:\n",
    "                thread = threading.Thread(target=k_working_epw_thread(k_works, states_can_be_visited, timestep, state, num_actions, current_qs, vi, env, horizon))\n",
    "                threads.append(thread)\n",
    "            # run the threads\n",
    "            for thread in threads:\n",
    "                thread.start()\n",
    "            # merge the results of the threads\n",
    "            for thread in threads:\n",
    "                thread.join()\n",
    "        if not k_works:\n",
    "            break\n",
    "    \n",
    "    if k_works:\n",
    "        return k\n",
    "        \n",
    "    # otherwise run bellman backup and up k\n",
    "    for timestep in tqdm(range(horizon)):\n",
    "        # thread across different states in a given timestep, so no overwriting vals in data structures\n",
    "        threads = []\n",
    "        for state in vi.visitable_states[timestep]:\n",
    "            thread = threading.Thread(target=epw_bellman_backup_thread())\n",
    "            threads.append(thread)\n",
    "        # run the threads\n",
    "        for thread in threads:\n",
    "            thread.start()\n",
    "        # merge the results of the threads\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratchwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "pong_table = np.load('/Users/laurenc/Documents/GitHub/282_expansion/data/bridge_dataset/mdps/pong_20_fs30/consolidated.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[123, 123,  70,  28,  70,  28],\n",
       "       [ 66,  66,  66, 242,  66, 242],\n",
       "       [138, 138, 138, 114, 138, 114],\n",
       "       ...,\n",
       "       [146, 146, 127, 117, 127, 117],\n",
       "       [ 65,  65,  65, 253,  65, 253],\n",
       "       [177, 177, 116, 147, 116, 147]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pong_table['transitions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
