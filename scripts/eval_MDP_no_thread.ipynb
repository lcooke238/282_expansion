{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import threading\n",
    "import math\n",
    "from time import sleep\n",
    "import queue\n",
    "\n",
    "# constants and structs\n",
    "class ViResults:\n",
    "    def __init__(self, exploration_qs, exploration_values, optimal_qs, optimal_values, worst_qs, worst_values, visitable_states):\n",
    "        self.exploration_qs = exploration_qs\n",
    "        self.exploration_values = exploration_values\n",
    "        self.optimal_qs = optimal_qs\n",
    "        self.optimal_values = optimal_values\n",
    "        self.worst_qs = worst_qs\n",
    "        self.worst_values = worst_values\n",
    "        self.visitable_states = visitable_states\n",
    "\n",
    "class EHResults:\n",
    "    def __init__(self, ks, ms, vars, gaps, effective_horizon):\n",
    "        self.ks = ks\n",
    "        self.ms = ms\n",
    "        self.vars = vars\n",
    "        self.gaps = gaps\n",
    "        self.effective_horizon = effective_horizon\n",
    "\n",
    "REWARD_PRECISION = 1e-4\n",
    "\n",
    "def findmax(arr):\n",
    "    max_val = np.max(arr)\n",
    "    max_index = np.argmax(arr)\n",
    "    return max_val, max_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pipeline: functions to get bounds and their helpers ###\n",
    "\n",
    "\n",
    "# value iteration: calculate exploration_qs, exploration_values, optimal_qs, optimal_values, worst_qs, worst_values, visitable_states, returns in single results structure in this order\n",
    "def value_iteration(transitions, rewards, horizon, exploration_policy=None):\n",
    "    # get constants from environment\n",
    "    num_states = len(transitions) #env.observation_space.n\n",
    "    num_actions = len(transitions[0]) #env.action_space.n\n",
    "\n",
    "    # get list of visitable states where each item is a set of visitable states at a given timestep (index)\n",
    "    visitable_states = []\n",
    "    current_visitable_states = set()\n",
    "    current_visitable_states.add(0)\n",
    "    for _ in tqdm(range(horizon)):\n",
    "        next_visitable_states = set()\n",
    "        for state in current_visitable_states:\n",
    "            for action in range(num_actions):\n",
    "                next_state = transitions[state][action]\n",
    "                next_visitable_states.add(next_state)\n",
    "        visitable_states.append(next_visitable_states.union(current_visitable_states))\n",
    "        current_visitable_states = next_visitable_states\n",
    "    print(len(visitable_states))\n",
    "    print(horizon)\n",
    "    \n",
    "    # initialize outputs\n",
    "    exploration_qs = np.full((horizon, num_states, num_actions), 0.)\n",
    "    exploration_values = np.full((horizon, num_states), 0.)\n",
    "    optimal_qs = np.full((horizon, num_states, num_actions), 0.)\n",
    "    optimal_values = np.full((horizon, num_states), 0.)\n",
    "    worst_qs = np.full((horizon, num_states, num_actions), 0.)\n",
    "    worst_values = np.full((horizon, num_states), 0.)\n",
    "\n",
    "    # get output vals for vi\n",
    "    for ts in tqdm(reversed(range(horizon))):\n",
    "        for state in visitable_states[ts]:\n",
    "            for action in range(num_actions):\n",
    "                obs = transitions[state][action]\n",
    "                reward = rewards[state][action]\n",
    "                if ts < horizon:\n",
    "                    # a = reward + exploration_values[ts+1, obs]\n",
    "                    # if a != 0:\n",
    "                    #     print(a)\n",
    "                    exploration_qs[ts, state, action] = reward + exploration_values[ts+1, obs]\n",
    "                    # if a != 0:\n",
    "                    #     print(exploration_qs[ts, state, action])\n",
    "                    optimal_qs[ts, state, action] = reward + optimal_values[ts+1, obs]\n",
    "                    worst_qs[ts, state, action] = reward + worst_values[ts+1, obs]\n",
    "                else:\n",
    "                    exploration_qs[ts, state, action] = reward\n",
    "                    optimal_qs[ts, state, action] = reward\n",
    "                    worst_qs[ts, state, action] = reward\n",
    "\n",
    "            optimal_value = max(optimal_qs[ts, state, :])\n",
    "            worst_value = min(worst_qs[ts, state, :])\n",
    "            if exploration_policy == None:\n",
    "                exploration_value = sum(exploration_qs[ts, state, :]) / num_actions\n",
    "            else:\n",
    "                exploration_value = 0\n",
    "                for action in range(num_actions):\n",
    "                    exploration_value += exploration_qs[ts, state, action] * exploration_policy[ts, state, action]\n",
    "            \n",
    "            optimal_values[ts, state] = optimal_value\n",
    "            worst_values[ts, state] = worst_value\n",
    "\n",
    "            # verification: no nans and no float errs\n",
    "            # ensure exploration val is bw worst and optimal vals (avoid floating pt errors)\n",
    "            exploration_values[ts, state] = min(optimal_value, max(worst_value, exploration_value))\n",
    "            assert(not np.isnan(exploration_values[ts, state]))\n",
    "            assert(not np.isnan(optimal_values[ts, state]))\n",
    "            assert(not np.isnan(worst_values[ts, state]))\n",
    "\n",
    "    # define results struct and return results\n",
    "    # assert(not np.isclose(exploration_values, np.full((horizon, num_states), 0.)).all())\n",
    "    results = ViResults(exploration_qs, exploration_values, optimal_qs, optimal_values, worst_qs, worst_values, visitable_states)\n",
    "    return results\n",
    "\n",
    "\n",
    "# calculate EH bound using GORP bounds\n",
    "def get_EH_bound(transitions, rewards, horizon, exploration_policy=None):\n",
    "    num_states = len(transitions)#env.observation_space.n\n",
    "    num_actions = len(transitions[0])#env.action_space.n\n",
    "    # Perform value iteration\n",
    "    vi = value_iteration(transitions, rewards, horizon, exploration_policy)\n",
    "    print(\"done with value iteration\")\n",
    "    \n",
    "    # initialize and compute variance bounds\n",
    "    var_bounds = np.full((horizon, num_states, num_actions), 0.)\n",
    "    for timestep in tqdm(range(horizon-1)):\n",
    "        # across different states in a given timestep, so no overwriting vals in data structures\n",
    "        for state in vi.visitable_states[timestep]:\n",
    "            # finds a bound on the variance of the qs based on the best and worst qs for each state at a timestep\n",
    "            for action in range(num_actions):\n",
    "                q = vi.exploration_qs[timestep, state, action]\n",
    "                worst_q = vi.worst_qs[timestep,state,action]\n",
    "                optimal_q = vi.optimal_qs[timestep, state, action]\n",
    "                var_bound = (q - worst_q) * (optimal_q - worst_q)\n",
    "                var_bounds[timestep, state, action] = var_bound\n",
    "    print(\"done with variance bounds\")\n",
    "\n",
    "    # initialize q values, starting k, and EH results structure\n",
    "    current_qs = vi.exploration_qs\n",
    "    results = EHResults([],[],[],[],horizon)\n",
    "    k = 1\n",
    "\n",
    "    # find best working k\n",
    "    while k < results.effective_horizon:\n",
    "        # initialize results objects and iteration\n",
    "        k_works = True\n",
    "        state_ms = np.full((num_states, horizon), 0.)\n",
    "        state_vars = np.full((num_states, horizon), 0.)\n",
    "        state_gaps = np.full((num_states, horizon), 0.)\n",
    "        states_can_be_visited = np.full((num_states, horizon), False)\n",
    "        states_can_be_visited[0,0] = True\n",
    "        \n",
    "        for timestep in tqdm(reversed(range(horizon-1))):\n",
    "            for state in vi.visitable_states[timestep]:\n",
    "                # checks k validity and adds to seeable next states in following timestep\n",
    "                if states_can_be_visited[timestep, state]:\n",
    "                    # init vals\n",
    "                    max_q = -np.inf\n",
    "                    max_suboptimal_q = -np.inf\n",
    "                    max_var = 0\n",
    "                    # iterate over actions to get actual vals\n",
    "                    for action in range(num_actions):\n",
    "                        q = current_qs[timestep, state, action]\n",
    "                        max_q = max(max_q, q)\n",
    "                        if vi.optimal_qs[timestep,state,action] < vi.optimal_values[timestep, state] - REWARD_PRECISION:\n",
    "                            max_suboptimal_q = max(max_suboptimal_q, q)\n",
    "                        max_var = max(max_var, var_bounds[timestep, state, action])\n",
    "                        # check for k fail condition\n",
    "                        if max_q == max_suboptimal_q:\n",
    "                               k_works = False\n",
    "                        # otherwise get the state m value\n",
    "                        else:\n",
    "                            gap = max_q - max_suboptimal_q\n",
    "                            state_gaps[timestep, state] = gap\n",
    "                            state_vars[timestep, state] = max_var\n",
    "                            m = math.ceil(16 * max_var / (gap**2) * math.log(2 * horizon * (num_actions**k)))\n",
    "                            state_ms[timestep, state] = max(1,m)\n",
    "                        # iterate through actions to find next visitable states\n",
    "                            for action in range(num_actions):\n",
    "                                if current_qs[timestep, state, action] > max_suboptimal_q:\n",
    "                                    next_state = transitions[state][action]\n",
    "                                    # _, next_state, _, _ = env.P[state][action]\n",
    "                                    if timestep < horizon:\n",
    "                                        states_can_be_visited[timestep+1, next_state] = True\n",
    "        if not k_works:\n",
    "            break\n",
    "        print(\"done with k check\")\n",
    "    \n",
    "        # if k works flag not triggered, then k is horizon and we update result vals accordingly\n",
    "        if k_works:\n",
    "            results.ks.append(k)\n",
    "            highest_m = np.max(state_ms)\n",
    "            timestep, state = np.unravel_index(np.argmax(state_ms), state_ms.shape)\n",
    "            results.ms.append(highest_m)\n",
    "            # log of highest_m with base num_actions\n",
    "            H_k = k + math.log(highest_m, num_actions)\n",
    "            print(f\"H_{k} = {H_k}\")\n",
    "            results.gaps.append(state_gaps[timestep, state])\n",
    "            results.vars.append(state_vars[timestep, state])\n",
    "            results.effective_horizon = min(results.effective_horizon, H_k)\n",
    "        \n",
    "        # run a bellman backup\n",
    "        for timestep in tqdm(reversed(range(horizon-1))):\n",
    "            # thread across different states in a given timestep, so no overwriting vals in data structures\n",
    "            for state in vi.visitable_states[timestep]:\n",
    "                 # update the current qs and variance bounds for the run\n",
    "                 for action in range(num_actions):\n",
    "                    next_state = transitions[state][action]\n",
    "                    reward = rewards[state][action]\n",
    "                    #_, next_state, reward, _ = env.P[state][action]\n",
    "                    max_next_q = -np.inf\n",
    "                    max_next_var_bound = 0\n",
    "                    for action in range(num_actions):\n",
    "                        next_q = current_qs[timestep+1, next_state, action]\n",
    "                        max_next_q = max(max_next_q, next_q)\n",
    "                        next_var_bound = var_bounds[timestep+1, next_state, action]\n",
    "                        max_next_var_bound = max(max_next_var_bound, next_var_bound)\n",
    "                    current_qs[timestep, state, action] = reward + max_next_q\n",
    "                    var_bounds[timestep, state, action] = max_next_var_bound\n",
    "        print(\"done w bellman backup\")        \n",
    "        k += 1\n",
    "        print(\"k increased\")\n",
    "    print(\"EH results done\")\n",
    "    return results\n",
    "\n",
    "\n",
    "# calculate EPW\n",
    "def get_EPW(transitions, rewards, horizon, exploration_policy=None, start_with_rewards=True):\n",
    "    # calculate min k for the gym environment and return it\n",
    "    num_states = len(transitions) # env.observation_space.n\n",
    "    num_actions = len(transitions[0]) # env.action_space.n\n",
    "\n",
    "    # Perform value iteration\n",
    "    vi = value_iteration(transitions, rewards, horizon, exploration_policy)\n",
    "\n",
    "    # init current_qs\n",
    "    if start_with_rewards:\n",
    "        current_qs = np.full((horizon, num_states, num_actions), 0.)\n",
    "        for timestep in tqdm(range(horizon)):\n",
    "            for state in vi.visitable_states[timestep]:\n",
    "                for action in range(num_actions):\n",
    "                    current_qs[timestep, state, action] = rewards[state][action]\n",
    "                    #_, _, current_qs[timestep, state, action], _ = env.P[state][action]\n",
    "    else:\n",
    "        current_qs = vi.exploration_qs\n",
    "\n",
    "    #\n",
    "    k = 1\n",
    "    while True:\n",
    "        # check if this k value works\n",
    "        k_works = True\n",
    "        states_can_be_visited = np.full((horizon, num_states), False)\n",
    "        states_can_be_visited[0,0] = True\n",
    "        for timestep in tqdm(range(horizon)):\n",
    "            for state in vi.visitable_states[timestep]:\n",
    "                #checks if given state works for k\n",
    "                if states_can_be_visited[timestep, state]:\n",
    "                    # init and find max q\n",
    "                    max_q = -np.inf\n",
    "                    for action in range(num_actions):\n",
    "                        max_q = max(max_q, current_qs[timestep, state, action])\n",
    "                    for action in range(num_actions):\n",
    "                        # check if possible to take this action\n",
    "                        if current_qs[timestep, state, action] >= max_q:\n",
    "                            if vi.optimal_qs[timestep, state, action] < vi.optimal_values[timestep,state] - REWARD_PRECISION:\n",
    "                                k_works = False\n",
    "                            next_state = transitions[state][action]\n",
    "                            #_, next_state, _, _ = env.P[state][action]\n",
    "                            if timestep < horizon:\n",
    "                                states_can_be_visited[timestep+1, next_state] = True\n",
    "        if not k_works:\n",
    "            break\n",
    "    \n",
    "    if k_works:\n",
    "        return k\n",
    "            \n",
    "    # otherwise run bellman backup and up k\n",
    "    for timestep in tqdm(range(horizon)):\n",
    "        for state in vi.visitable_states[timestep]:\n",
    "            for action in range(num_actions):\n",
    "                next_state = transitions[state][action]\n",
    "                reward = rewards[state][action]\n",
    "                #_, next_state, reward, _ = env.P[state][action]\n",
    "                max_next_q = -np.inf\n",
    "                for action in range(num_actions):\n",
    "                    next_q = current_qs[timestep+1, next_state, action]\n",
    "                    max_next_q = max(max_next_q, next_q)\n",
    "                current_qs[timestep, state, action] = reward + max_next_q\n",
    "\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123 123 70 28 70 28 ;\n",
      "66 66 66 242 66 242 ;\n",
      "138 138 138 114 138 114 ;\n",
      "41 41 41 20 41 20 ;\n",
      "209 209 209 108 209 108 ;\n",
      "140 140 170 239 170 239 ;\n",
      "54 54 185 122 185 122 ;\n",
      "143 143 24 143 24 143 ;\n",
      "19 19 19 173 19 173 ;\n",
      "67 67 67 202 67 202 ;\n",
      "16 16 45 71 45 71 ;\n",
      "37 37 65 77 65 77 ;\n",
      "53 53 159 141 159 141 ;\n",
      "86 86 219 217 219 217 ;\n",
      "184 184 2 63 2 63 ;\n",
      "150 150 150 204 150 204 ;\n",
      "215 215 167 50 167 50 ;\n",
      "-1 -1 -1 -1 -1 -1 ;\n",
      "72 72 178 145 178 145 ;\n",
      "41 41 41 20 41 20 ;\n",
      "-1 -1 -1 -1 -1 -1 ;\n",
      "157 157 94 157 94 157 ;\n",
      "238 238 66 82 66 82 ;\n",
      "75 75 160 157 160 157 ;\n",
      "-1 -1 -1 -1 -1 -1 ;\n",
      "57 57 19 225 19 225 ;\n",
      "-1 -1 -1 -1 -1 -1 ;\n",
      "112 112 198 112 198 112 ;\n",
      "49 49 11 49 11 49 ;\n",
      "161 161 235 7 235 7 ;\n",
      "101 101 168 101 168 101 ;\n",
      "35 35 35 39 35 39 ;\n",
      "72 72 178 145 178 145 ;\n",
      "86 86 219 217 219 217 ;\n",
      "170 170 170 32 170 32 ;\n",
      "-1 -1 -1 -1 -1 -1 ;\n",
      "143 143 24 143 24 143 ;\n",
      "16 16 45 71 45 71 ;\n",
      "44 44 56 74 56 74 ;\n",
      "-1 -1 -1 -1 -1 -1 ;\n",
      "154 154 148 27 148 27 ;\n",
      "-1 -1 -1 -1 -1 -1 ;\n",
      "97 97 189 201 189 201 ;\n",
      "98 98 81 153 81 153 ;\n",
      "18 18 188 239 188 239 ;\n",
      "167 167 167 234 167 234 ;\n",
      "91 91 91 29 91 29 ;\n",
      "183 183 183 38 183 38 ;\n",
      "64 64 180 64 180 64 ;\n",
      "166 166 10 166 10 166 ;\n",
      "141 141 194 141 194 141 ;\n",
      "42 244 3 199 3 199 ;\n",
      "145 145 195 145 195 145 ;\n",
      "142 142 183 122 183 122 ;\n",
      "44 44 56 74 56 74 ;\n",
      "137 137 68 36 68 36 ;\n",
      "170 170 170 32 170 32 ;\n",
      "41 41 41 175 41 175 ;\n",
      "31 31 208 236 208 236 ;\n",
      "211 211 34 176 34 176 ;\n",
      "191 191 191 240 191 240 ;\n",
      "132 132 227 111 227 111 ;\n",
      "220 220 159 85 159 85 ;\n",
      "83 83 216 83 216 83 ;\n",
      "217 217 55 217 55 217 ;\n",
      "45 45 45 165 45 165 ;\n",
      "138 138 138 114 138 114 ;\n",
      "-1 -1 -1 -1 -1 -1 ;\n",
      "67 67 67 152 67 152 ;\n",
      "104 104 8 250 8 250 ;\n",
      "252 252 252 214 252 214 ;\n",
      "88 88 62 88 62 88 ;\n",
      "75 75 160 157 160 157 ;\n",
      "227 227 227 33 227 33 ;\n",
      "239 239 187 239 187 239 ;\n",
      "230 230 226 112 226 112 ;\n",
      "208 208 208 99 208 99 ;\n",
      "147 147 169 147 169 147 ;\n",
      "134 134 150 223 150 223 ;\n",
      "22 22 150 171 150 171 ;\n",
      "137 137 68 36 68 36 ;\n",
      "-1 -1 -1 -1 -1 -1 ;\n",
      "83 83 216 83 216 83 ;\n",
      "250 250 25 250 25 250 ;\n",
      "40 40 248 21 248 21 ;\n",
      "122 122 59 122 59 122 ;\n",
      "161 161 235 7 235 7 ;\n",
      "4 4 4 131 4 131 ;\n",
      "141 141 194 141 194 141 ;\n",
      "15 15 15 113 15 113 ;\n",
      "221 221 89 112 89 112 ;\n",
      "68 68 68 207 68 207 ;\n",
      "115 115 138 105 138 105 ;\n",
      "57 57 19 225 19 225 ;\n",
      "243 243 89 218 89 218 ;\n",
      "-1 -1 -1 -1 -1 -1 ;\n",
      "-1 -1 -1 -1 -1 -1 ;\n",
      "-1 -1 -1 -1 -1 -1 ;\n",
      "-1 -1 -1 -1 -1 -1 ;\n",
      "98 98 81 153 81 153 ;\n",
      "193 193 120 250 120 250 ;\n",
      "245 245 174 245 174 245 ;\n",
      "43 43 136 237 136 237 ;\n",
      "84 84 170 52 170 52 ;\n",
      "110 110 19 199 19 199 ;\n",
      "250 250 25 250 25 250 ;\n",
      "107 107 228 101 228 101 ;\n",
      "102 102 76 245 76 245 ;\n",
      "43 43 136 237 136 237 ;\n",
      "-1 -1 -1 -1 -1 -1 ;\n",
      "109 109 41 229 41 229 ;\n",
      "217 217 55 217 55 217 ;\n",
      "210 210 222 210 222 210 ;\n",
      "14 14 1 223 1 223 ;\n",
      "193 193 120 250 120 250 ;\n",
      "93 93 8 233 8 233 ;\n",
      "167 167 167 234 167 234 ;\n",
      "30 30 128 30 128 30 ;\n",
      "-1 -1 -1 -1 -1 -1 ;\n",
      "90 90 148 157 148 157 ;\n",
      "19 19 19 173 19 173 ;\n",
      "163 163 45 147 45 147 ;\n",
      "74 74 103 74 103 74 ;\n",
      "190 190 252 49 252 49 ;\n",
      "4 4 4 131 4 131 ;\n",
      "-1 -1 -1 -1 -1 -1 ;\n",
      "241 251 196 210 196 210 ;\n",
      "124 124 124 139 124 139 ;\n",
      "164 164 4 200 4 200 ;\n",
      "-1 -1 -1 -1 -1 -1 ;\n",
      "48 48 61 48 61 48 ;\n",
      "102 102 76 245 76 245 ;\n",
      "80 80 91 158 91 158 ;\n",
      "179 179 47 141 47 141 ;\n",
      "205 205 66 63 66 63 ;\n",
      "149 149 73 48 73 48 ;\n",
      "35 35 35 129 35 129 ;\n",
      "67 67 67 202 67 202 ;\n",
      "8 8 8 51 8 51 ;\n",
      "107 107 228 101 228 101 ;\n",
      "119 119 248 145 248 145 ;\n",
      "122 122 59 122 59 122 ;\n",
      "5 5 34 74 34 74 ;\n",
      "-1 -1 -1 -1 -1 -1 ;\n",
      "74 74 103 74 103 74 ;\n",
      "157 157 94 157 94 157 ;\n",
      "106 106 87 30 87 30 ;\n",
      "88 88 62 88 62 88 ;\n",
      "89 89 89 126 89 126 ;\n",
      "13 13 46 64 46 64 ;\n",
      "66 66 66 242 66 242 ;\n",
      "-1 -1 -1 -1 -1 -1 ;\n",
      "-1 -1 -1 -1 -1 -1 ;\n",
      "-1 -1 -1 -1 -1 -1 ;\n",
      "243 243 89 218 89 218 ;\n",
      "-1 -1 -1 -1 -1 -1 ;\n",
      "45 45 45 165 45 165 ;\n",
      "112 112 198 112 198 112 ;\n",
      "7 7 9 7 9 7 ;\n",
      "183 183 183 38 183 38 ;\n",
      "89 89 89 126 89 126 ;\n",
      "162 162 96 143 96 143 ;\n",
      "-1 -1 -1 -1 -1 -1 ;\n",
      "12 12 167 88 167 88 ;\n",
      "58 58 209 232 209 232 ;\n",
      "133 133 212 88 212 88 ;\n",
      "147 147 169 147 169 147 ;\n",
      "159 159 159 6 159 6 ;\n",
      "58 58 209 232 209 232 ;\n",
      "215 215 167 50 167 50 ;\n",
      "248 248 248 23 248 23 ;\n",
      "63 63 92 63 92 63 ;\n",
      "18 18 188 239 188 239 ;\n",
      "151 151 26 229 26 229 ;\n",
      "31 31 208 236 208 236 ;\n",
      "-1 -1 -1 -1 -1 -1 ;\n",
      "239 239 187 239 187 239 ;\n",
      "133 133 212 88 212 88 ;\n",
      "148 148 148 224 148 224 ;\n",
      "54 54 185 122 185 122 ;\n",
      "80 80 91 158 91 158 ;\n",
      "8 8 8 51 8 51 ;\n",
      "211 211 34 176 34 176 ;\n",
      "34 34 34 172 34 172 ;\n",
      "100 100 181 83 181 83 ;\n",
      "34 34 34 172 34 172 ;\n",
      "-1 -1 -1 -1 -1 -1 ;\n",
      "40 40 248 21 248 21 ;\n",
      "248 248 248 23 248 23 ;\n",
      "-1 -1 -1 -1 -1 -1 ;\n",
      "121 121 65 166 65 166 ;\n",
      "227 227 227 33 227 33 ;\n",
      "223 223 213 223 213 223 ;\n",
      "42 244 3 199 3 199 ;\n",
      "182 182 183 144 183 144 ;\n",
      "154 154 148 27 148 27 ;\n",
      "150 150 150 204 150 204 ;\n",
      "125 125 125 203 125 203 ;\n",
      "79 79 15 192 15 192 ;\n",
      "229 229 155 229 155 229 ;\n",
      "245 245 174 245 174 245 ;\n",
      "-1 -1 -1 -1 -1 -1 ;\n",
      "-1 -1 -1 -1 -1 -1 ;\n",
      "-1 -1 -1 -1 -1 -1 ;\n",
      "184 184 2 63 2 63 ;\n",
      "69 69 138 83 138 83 ;\n",
      "177 177 116 147 116 147 ;\n",
      "162 162 96 143 96 143 ;\n",
      "35 35 35 129 35 129 ;\n",
      "208 208 208 99 208 99 ;\n",
      "223 223 213 223 213 223 ;\n",
      "84 84 170 52 170 52 ;\n",
      "159 159 159 6 159 6 ;\n",
      "238 238 66 82 66 82 ;\n",
      "206 206 156 166 156 166 ;\n",
      "220 220 159 85 159 85 ;\n",
      "93 93 8 233 8 233 ;\n",
      "7 7 9 7 9 7 ;\n",
      "210 210 222 210 222 210 ;\n",
      "68 68 68 207 68 207 ;\n",
      "182 182 183 144 183 144 ;\n",
      "78 78 15 210 15 210 ;\n",
      "22 22 150 171 150 171 ;\n",
      "63 63 92 63 92 63 ;\n",
      "230 230 226 112 226 112 ;\n",
      "229 229 155 229 155 229 ;\n",
      "15 15 15 113 15 113 ;\n",
      "91 91 91 29 91 29 ;\n",
      "209 209 209 108 209 108 ;\n",
      "-1 -1 -1 -1 -1 -1 ;\n",
      "241 251 196 210 196 210 ;\n",
      "35 35 35 39 35 39 ;\n",
      "237 237 231 237 231 237 ;\n",
      "199 199 197 199 247 199 ;\n",
      "179 179 47 141 47 141 ;\n",
      "67 67 67 152 67 152 ;\n",
      "153 153 246 153 246 153 ;\n",
      "153 153 246 153 246 153 ;\n",
      "115 115 138 105 138 105 ;\n",
      "145 145 195 145 195 145 ;\n",
      "13 13 46 64 46 64 ;\n",
      "135 135 60 130 60 130 ;\n",
      "100 100 181 83 181 83 ;\n",
      "79 79 15 192 15 192 ;\n",
      "95 95 249 17 249 17 ;\n",
      "237 237 231 237 231 237 ;\n",
      "-1 -1 -1 -1 -1 -1 ;\n",
      "118 118 118 186 118 186 ;\n",
      "148 148 148 224 148 224 ;\n",
      "-1 -1 -1 -1 -1 -1 ;\n",
      "199 199 197 199 247 199 ;\n",
      "146 146 127 117 127 117 ;\n",
      "65 65 65 253 65 253 ;\n",
      "177 177 116 147 116 147 ;\n"
     ]
    }
   ],
   "source": [
    "## run through ##\n",
    "\n",
    "# load tables from dataset\n",
    "pong_table = np.load('/Users/laurenc/Documents/GitHub/282_expansion/data/bridge_dataset/mdps/pong_20_fs30/consolidated.npz')\n",
    "rewards = pong_table['rewards']\n",
    "transitions = pong_table['transitions']\n",
    "for line in transitions:\n",
    "    for item in line:\n",
    "        print(item, end=\" \")\n",
    "    print(\";\")\n",
    "horizon = len(transitions)\n",
    "\n",
    "# calculate eh\n",
    "# eh_results = get_EH_bound(transitions, rewards, horizon)\n",
    "# print(eh_results.effective_horizon)\n",
    "\n",
    "# calculate EPW\n",
    "# epw_result = get_EPW(transitions, rewards, horizon)\n",
    "# print(epw_result)\n",
    "\n",
    "# # print results\n",
    "# print(f\"effective horizon: {eh_results.effective_horizon} \\n effective planning window: {epw_result}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
