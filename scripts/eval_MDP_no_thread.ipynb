{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import threading\n",
    "import math\n",
    "from time import sleep\n",
    "import queue\n",
    "\n",
    "# constants and structs\n",
    "class ViResults:\n",
    "    def __init__(self, exploration_qs, exploration_values, optimal_qs, optimal_values, worst_qs, worst_values, visitable_states):\n",
    "        self.exploration_qs = exploration_qs\n",
    "        self.exploration_values = exploration_values\n",
    "        self.optimal_qs = optimal_qs\n",
    "        self.optimal_values = optimal_values\n",
    "        self.worst_qs = worst_qs\n",
    "        self.worst_values = worst_values\n",
    "        self.visitable_states = visitable_states\n",
    "\n",
    "class EHResults:\n",
    "    def __init__(self, ks, ms, vars, gaps, effective_horizon):\n",
    "        self.ks = ks\n",
    "        self.ms = ms\n",
    "        self.vars = vars\n",
    "        self.gaps = gaps\n",
    "        self.effective_horizon = effective_horizon\n",
    "\n",
    "REWARD_PRECISION = 1e-4\n",
    "\n",
    "def findmax(arr):\n",
    "    max_val = np.max(arr)\n",
    "    max_index = np.argmax(arr)\n",
    "    return max_val, max_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pipeline: functions to get bounds and their helpers ###\n",
    "\n",
    "\n",
    "# value iteration: calculate exploration_qs, exploration_values, optimal_qs, optimal_values, worst_qs, worst_values, visitable_states, returns in single results structure in this order\n",
    "def value_iteration(transitions, rewards, horizon, exploration_policy=None):\n",
    "    # get constants from environment\n",
    "    num_states = len(transitions) #env.observation_space.n\n",
    "    num_actions = len(transitions[0]) #env.action_space.n\n",
    "\n",
    "    # get list of visitable states where each item is a set of visitable states at a given timestep (index)\n",
    "    visitable_states = []\n",
    "    current_visitable_states = set()\n",
    "    current_visitable_states.add(0)\n",
    "    for _ in tqdm(range(horizon)):\n",
    "        next_visitable_states = set()\n",
    "        for state in current_visitable_states:\n",
    "            for action in range(num_actions):\n",
    "                next_state = transitions[state][action]\n",
    "                next_visitable_states.add(next_state)\n",
    "        visitable_states.append(next_visitable_states.union(current_visitable_states))\n",
    "        current_visitable_states = next_visitable_states\n",
    "    \n",
    "    # initialize outputs \n",
    "    exploration_qs = np.full((horizon, num_states, num_actions), np.nan)\n",
    "    exploration_values = np.full((horizon, num_states), np.nan)\n",
    "    optimal_qs = np.full((horizon, num_states, num_actions), np.nan)\n",
    "    optimal_values = np.full((horizon, num_states), np.nan)\n",
    "    worst_qs = np.full((horizon, num_states, num_actions), np.nan)\n",
    "    worst_values = np.full((horizon, num_states), np.nan)\n",
    "\n",
    "    # get output vals for vi\n",
    "    for ts in tqdm(reversed(range(horizon))):\n",
    "        for state in visitable_states[ts]:\n",
    "            for action in range(num_actions):\n",
    "                obs = transitions[state][action]\n",
    "                reward = rewards[state][action]\n",
    "                if ts < horizon-1:\n",
    "                    exploration_qs[ts, state, action] = reward + exploration_values[ts+1, obs]\n",
    "                    optimal_qs[ts, state, action] = reward + optimal_values[ts+1, obs]\n",
    "                    worst_qs[ts, state, action] = reward + worst_values[ts+1, obs]\n",
    "                else:\n",
    "                    exploration_qs[ts, state, action] = reward\n",
    "                    optimal_qs[ts, state, action] = reward\n",
    "                    worst_qs[ts, state, action] = reward\n",
    "\n",
    "            optimal_value = max(optimal_qs[ts, state, :])\n",
    "            worst_value = min(worst_qs[ts, state, :])\n",
    "            if exploration_policy == None:\n",
    "                exploration_value = sum(exploration_qs[ts, state, :]) / num_actions\n",
    "            else:\n",
    "                exploration_value = 0\n",
    "                for action in range(num_actions):\n",
    "                    exploration_value += exploration_qs[ts, state, action] * exploration_policy[ts, state, action]\n",
    "            \n",
    "            optimal_values[ts, state] = optimal_value\n",
    "            worst_values[ts, state] = worst_value\n",
    "\n",
    "            # verification: no nans and no float errs\n",
    "            # ensure exploration val is bw worst and optimal vals (avoid floating pt errors)\n",
    "            exploration_values[ts, state] = min(optimal_value, max(worst_value, exploration_value))\n",
    "            assert(not np.isnan(exploration_values[ts, state]))\n",
    "            assert(not np.isnan(optimal_values[ts, state]))\n",
    "            assert(not np.isnan(worst_values[ts, state]))\n",
    "\n",
    "    # define results struct and return results\n",
    "    # assert(not np.isclose(exploration_values, np.full((horizon, num_states), 0.)).all())\n",
    "    results = ViResults(exploration_qs, exploration_values, optimal_qs, optimal_values, worst_qs, worst_values, visitable_states)\n",
    "    return results\n",
    "\n",
    "\n",
    "# calculate EH bound using GORP bounds\n",
    "def get_EH_bound(transitions, rewards, horizon, exploration_policy=None):\n",
    "    num_states = len(transitions)#env.observation_space.n\n",
    "    num_actions = len(transitions[0])#env.action_space.n\n",
    "    # Perform value iteration\n",
    "    vi = value_iteration(transitions, rewards, horizon, exploration_policy)\n",
    "    print(\"done with value iteration\")\n",
    "    \n",
    "    # initialize and compute variance bounds\n",
    "    var_bounds = np.full((horizon, num_states, num_actions), 0.)\n",
    "    for timestep in tqdm(range(horizon)):\n",
    "        # across different states in a given timestep, so no overwriting vals in data structures\n",
    "        for state in vi.visitable_states[timestep]:\n",
    "            # finds a bound on the variance of the qs based on the best and worst qs for each state at a timestep\n",
    "            for action in range(num_actions):\n",
    "                q = vi.exploration_qs[timestep, state, action]\n",
    "                worst_q = vi.worst_qs[timestep,state,action]\n",
    "                optimal_q = vi.optimal_qs[timestep, state, action]\n",
    "                var_bound = (q - worst_q) * (optimal_q - worst_q)\n",
    "                var_bounds[timestep, state, action] = var_bound\n",
    "    print(\"done with variance bounds\")\n",
    "\n",
    "    # initialize q values, starting k, and EH results structure\n",
    "    current_qs = vi.exploration_qs\n",
    "    results = EHResults([],[],[],[],horizon)\n",
    "    k = 1\n",
    "\n",
    "    # find best working k\n",
    "    while k < results.effective_horizon:\n",
    "        # initialize results objects and iteration\n",
    "        k_works = True\n",
    "        state_ms = np.full((horizon, num_states), 0.)\n",
    "        state_vars = np.full((horizon, num_states), 0.)\n",
    "        state_gaps = np.full((horizon, num_states), 0.)\n",
    "        states_can_be_visited = np.full((horizon, num_states), False)\n",
    "        states_can_be_visited[0,0] = True\n",
    "        \n",
    "        for timestep in tqdm(reversed(range(horizon))):\n",
    "            for state in vi.visitable_states[timestep]:\n",
    "                # checks k validity and adds to seeable next states in following timestep\n",
    "                if states_can_be_visited[timestep, state]:\n",
    "                    # init vals\n",
    "                    max_q = -np.inf\n",
    "                    max_suboptimal_q = -np.inf\n",
    "                    max_var = 0\n",
    "                    # iterate over actions to get actual vals\n",
    "                    for action in range(num_actions):\n",
    "                        q = current_qs[timestep, state, action]\n",
    "                        max_q = max(max_q, q)\n",
    "                        if vi.optimal_qs[timestep,state,action] < vi.optimal_values[timestep, state] - REWARD_PRECISION:\n",
    "                            max_suboptimal_q = max(max_suboptimal_q, q)\n",
    "                        max_var = max(max_var, var_bounds[timestep, state, action])\n",
    "                        # check for k fail condition\n",
    "                        if max_q == max_suboptimal_q:\n",
    "                               k_works = False\n",
    "                        # otherwise get the state m value\n",
    "                        else:\n",
    "                            gap = max_q - max_suboptimal_q\n",
    "                            state_gaps[timestep, state] = gap\n",
    "                            state_vars[timestep, state] = max_var\n",
    "                            m = math.ceil(16 * max_var / (gap**2) * math.log(2 * horizon * (num_actions**k)))\n",
    "                            state_ms[timestep, state] = max(1,m)\n",
    "                        # iterate through actions to find next visitable states\n",
    "                            for action in range(num_actions):\n",
    "                                if current_qs[timestep, state, action] > max_suboptimal_q:\n",
    "                                    next_state = transitions[state][action]\n",
    "                                    # _, next_state, _, _ = env.P[state][action]\n",
    "                                    if timestep < horizon-1:\n",
    "                                        states_can_be_visited[timestep+1, next_state] = True\n",
    "        if not k_works:\n",
    "            break\n",
    "        print(\"done with k check\")\n",
    "    \n",
    "        # if k works flag not triggered, then k is horizon and we update result vals accordingly\n",
    "        if k_works:\n",
    "            results.ks.append(k)\n",
    "            highest_m = np.max(state_ms)\n",
    "            timestep, state = np.unravel_index(np.argmax(state_ms), state_ms.shape)\n",
    "            results.ms.append(highest_m)\n",
    "            # log of highest_m with base num_actions\n",
    "            H_k = k + math.log(highest_m, num_actions)\n",
    "            print(f\"H_{k} = {H_k}\")\n",
    "            results.gaps.append(state_gaps[timestep, state])\n",
    "            results.vars.append(state_vars[timestep, state])\n",
    "            results.effective_horizon = min(results.effective_horizon, H_k)\n",
    "        \n",
    "        # run a bellman backup\n",
    "        for timestep in tqdm(reversed(range(horizon-1))):\n",
    "            # thread across different states in a given timestep, so no overwriting vals in data structures\n",
    "            for state in vi.visitable_states[timestep]:\n",
    "                 # update the current qs and variance bounds for the run\n",
    "                 for action in range(num_actions):\n",
    "                    next_state = transitions[state][action]\n",
    "                    reward = rewards[state][action]\n",
    "                    #_, next_state, reward, _ = env.P[state][action]\n",
    "                    max_next_q = -np.inf\n",
    "                    max_next_var_bound = 0\n",
    "                    for action in range(num_actions):\n",
    "                        next_q = current_qs[timestep+1, next_state, action]\n",
    "                        max_next_q = max(max_next_q, next_q)\n",
    "                        next_var_bound = var_bounds[timestep+1, next_state, action]\n",
    "                        max_next_var_bound = max(max_next_var_bound, next_var_bound)\n",
    "                    current_qs[timestep, state, action] = reward + max_next_q\n",
    "                    var_bounds[timestep, state, action] = max_next_var_bound\n",
    "        print(\"done w bellman backup\")        \n",
    "        k += 1\n",
    "        print(\"k increased\")\n",
    "    print(\"EH results done\")\n",
    "    return results\n",
    "\n",
    "\n",
    "# calculate EPW\n",
    "def get_EPW(transitions, rewards, horizon, exploration_policy=None, start_with_rewards=True):\n",
    "    # calculate min k for the gym environment and return it\n",
    "    num_states = len(transitions) # env.observation_space.n\n",
    "    num_actions = len(transitions[0]) # env.action_space.n\n",
    "\n",
    "    # Perform value iteration\n",
    "    vi = value_iteration(transitions, rewards, horizon, exploration_policy)\n",
    "\n",
    "    # init current_qs\n",
    "    if start_with_rewards:\n",
    "        current_qs = np.full((horizon, num_states, num_actions), 0.)\n",
    "        for timestep in tqdm(range(horizon)):\n",
    "            for state in vi.visitable_states[timestep]:\n",
    "                for action in range(num_actions):\n",
    "                    current_qs[timestep, state, action] = rewards[state][action]\n",
    "                    #_, _, current_qs[timestep, state, action], _ = env.P[state][action]\n",
    "    else:\n",
    "        current_qs = vi.exploration_qs\n",
    "\n",
    "    #\n",
    "    k = 1\n",
    "    while True:\n",
    "        # check if this k value works\n",
    "        k_works = True\n",
    "        states_can_be_visited = np.full((horizon, num_states), False)\n",
    "        states_can_be_visited[0,0] = True\n",
    "        for timestep in tqdm(range(horizon)):\n",
    "            for state in vi.visitable_states[timestep]:\n",
    "                #checks if given state works for k\n",
    "                if states_can_be_visited[timestep, state]:\n",
    "                    # init and find max q\n",
    "                    max_q = -np.inf\n",
    "                    for action in range(num_actions):\n",
    "                        max_q = max(max_q, current_qs[timestep, state, action])\n",
    "                    for action in range(num_actions):\n",
    "                        # check if possible to take this action\n",
    "                        if current_qs[timestep, state, action] >= max_q:\n",
    "                            if vi.optimal_qs[timestep, state, action] < vi.optimal_values[timestep,state] - REWARD_PRECISION:\n",
    "                                k_works = False\n",
    "                            next_state = transitions[state][action]\n",
    "                            #_, next_state, _, _ = env.P[state][action]\n",
    "                            if timestep < horizon-1:\n",
    "                                states_can_be_visited[timestep+1, next_state] = True\n",
    "        if not k_works:\n",
    "            break\n",
    "    \n",
    "    if k_works:\n",
    "        return k\n",
    "            \n",
    "    # otherwise run bellman backup and up k\n",
    "    for timestep in tqdm(range(horizon)):\n",
    "        for state in vi.visitable_states[timestep]:\n",
    "            for action in range(num_actions):\n",
    "                next_state = transitions[state][action]\n",
    "                reward = rewards[state][action]\n",
    "                #_, next_state, reward, _ = env.P[state][action]\n",
    "                max_next_q = -np.inf\n",
    "                for action in range(num_actions):\n",
    "                    next_q = current_qs[timestep+1, next_state, action]\n",
    "                    max_next_q = max(max_next_q, next_q)\n",
    "                current_qs[timestep, state, action] = reward + max_next_q\n",
    "\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 25350.89it/s]\n",
      "20it [00:00, 1647.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with value iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 6241.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with variance bounds\n",
      "0.5861887211649242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:00, 68702.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with k check\n",
      "H_1 = 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:00, 1334.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done w bellman backup\n",
      "k increased\n",
      "EH results done\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## run through ##\n",
    "\n",
    "# load tables from dataset\n",
    "pong_table = np.load('/Users/laurenc/Documents/GitHub/282_expansion/data/bridge_dataset/mdps/pong_20_fs30/consolidated.npz')\n",
    "rewards = pong_table['rewards']\n",
    "transitions = pong_table['transitions']\n",
    "# for line in transitions:\n",
    "#     for item in line:\n",
    "#         print(item, end=\" \")\n",
    "#     print(\";\")\n",
    "horizon = 20\n",
    "\n",
    "# value iteration\n",
    "# vi = value_iteration(transitions, rewards, horizon, exploration_policy=None)\n",
    "# print(vi.exploration_qs[0][0][1])\n",
    "# for line in vi.exploration_qs:\n",
    "#     for item in line:\n",
    "#         print(item, end=\" \")\n",
    "#     print()\n",
    "# print()\n",
    "\n",
    "# calculate eh\n",
    "eh_results = get_EH_bound(transitions, rewards, horizon)\n",
    "print(eh_results.effective_horizon)\n",
    "\n",
    "# calculate EPW\n",
    "# epw_result = get_EPW(transitions, rewards, horizon)\n",
    "# print(epw_result)\n",
    "\n",
    "# # print results\n",
    "# print(f\"effective horizon: {eh_results.effective_horizon} \\n effective planning window: {epw_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
