{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import threading\n",
    "import math\n",
    "from time import sleep\n",
    "import queue\n",
    "\n",
    "# constants and structs\n",
    "class ViResults:\n",
    "    def __init__(self, exploration_qs, exploration_values, optimal_qs, optimal_values, worst_qs, worst_values, visitable_states):\n",
    "        self.exploration_qs = exploration_qs\n",
    "        self.exploration_values = exploration_values\n",
    "        self.optimal_qs = optimal_qs\n",
    "        self.optimal_values = optimal_values\n",
    "        self.worst_qs = worst_qs\n",
    "        self.worst_values = worst_values\n",
    "        self.visitable_states = visitable_states\n",
    "\n",
    "class EHResults:\n",
    "    def __init__(self, ks, ms, vars, gaps, effective_horizon):\n",
    "        self.ks = ks\n",
    "        self.ms = ms\n",
    "        self.vars = vars\n",
    "        self.gaps = gaps\n",
    "        self.effective_horizon = effective_horizon\n",
    "\n",
    "REWARD_PRECISION = 1e-4\n",
    "\n",
    "# setup lock and findmax\n",
    "lock = threading.Lock()\n",
    "\n",
    "def findmax(arr):\n",
    "    max_val = np.max(arr)\n",
    "    max_index = np.argmax(arr)\n",
    "    return max_val, max_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pipeline: functions to get bounds and their helpers ###\n",
    "\n",
    "\n",
    "# value iteration: calculate exploration_qs, exploration_values, optimal_qs, optimal_values, worst_qs, worst_values, visitable_states, returns in single results structure in this order\n",
    "def value_iteration(transitions, rewards, horizon, exploration_policy=None):\n",
    "    # get constants from environment\n",
    "    num_states = len(transitions) #env.observation_space.n\n",
    "    num_actions = len(transitions[0]) #env.action_space.n\n",
    "\n",
    "    # get list of visitable states where each item is a set of visitable states at a given timestep (index)\n",
    "    visitable_states = []\n",
    "    current_visitable_states = set()\n",
    "    current_visitable_states.add(0)\n",
    "    for _ in tqdm(range(horizon)):\n",
    "        next_visitable_states = set()\n",
    "        for state in current_visitable_states:\n",
    "            for action in range(num_actions):\n",
    "                next_state = transitions[state][action]\n",
    "                next_visitable_states.add(next_state)\n",
    "        visitable_states.append(next_visitable_states.union(current_visitable_states))\n",
    "        current_visitable_states = next_visitable_states\n",
    "    \n",
    "    # initialize outputs\n",
    "    exploration_qs = np.full((horizon, num_states, num_actions), 0)\n",
    "    exploration_values = np.full((horizon, num_states), 0)\n",
    "    optimal_qs = np.full((horizon, num_states, num_actions), 0)\n",
    "    optimal_values = np.full((horizon, num_states), 0)\n",
    "    worst_qs = np.full((horizon, num_states, num_actions), 0)\n",
    "    worst_values = np.full((horizon, num_states), 0)\n",
    "\n",
    "    # get output vals for vi\n",
    "    for ts in tqdm(range(horizon-1)):\n",
    "        for state in visitable_states[ts]:\n",
    "            for action in range(num_actions):\n",
    "                obs = transitions[state][action]\n",
    "                reward = rewards[state][action]\n",
    "                if ts < horizon:\n",
    "                    exploration_qs[ts, state, action] = reward + exploration_values[ts+1, obs]\n",
    "                    optimal_qs[ts, state, action] = reward + optimal_values[ts+1, obs]\n",
    "                    worst_qs[ts, state, action] = reward + worst_values[ts+1, obs]\n",
    "                else:\n",
    "                    exploration_qs[ts, state, action] = reward\n",
    "                    optimal_qs[ts, state, action] = reward\n",
    "                    worst_qs[ts, state, action] = reward\n",
    "\n",
    "            optimal_value = max(optimal_qs[ts, state, :])\n",
    "            worst_value = min(worst_qs[ts, state, :])\n",
    "            if exploration_policy == None:\n",
    "                exploration_value = sum(exploration_qs[ts, state, :]) / num_actions\n",
    "            else:\n",
    "                exploration_value = 0\n",
    "                for action in range(num_actions):\n",
    "                    exploration_value += exploration_qs[ts, state, action] * exploration_policy[ts, state, action]\n",
    "            \n",
    "            optimal_values[ts, state] = optimal_value\n",
    "            worst_values[ts, state] = worst_value\n",
    "\n",
    "            # verification: no nans and no float errs\n",
    "            # ensure exploration val is bw worst and optimal vals (avoid floating pt errors)\n",
    "            exploration_values[ts, state] = min(optimal_value, max(worst_value, exploration_value))\n",
    "            assert(not np.isnan(exploration_values[ts, state]))\n",
    "            assert(not np.isnan(optimal_values[ts, state]))\n",
    "            assert(not np.isnan(worst_values[ts, state]))\n",
    "\n",
    "    # define results struct and return results\n",
    "    results = ViResults(exploration_qs, exploration_values, optimal_qs, optimal_values, worst_qs, worst_values, visitable_states)\n",
    "    return results\n",
    "\n",
    "\n",
    "# compute variance bounds: finds a bound on the variance of the qs based on the best and worst qs for each state at a timestep\n",
    "def compute_variance_thread(var_bounds, ts, state, num_actions, vi):\n",
    "    for action in range(num_actions):\n",
    "        q = vi.exploration_qs[ts, state, action]\n",
    "        worst_q = vi.worst_qs[ts,state,action]\n",
    "        optimal_q = vi.optimal_qs[ts, state, action]\n",
    "        var_bound = (q - worst_q) * (optimal_q - worst_q)\n",
    "        var_bounds[ts, state, action] = var_bound\n",
    "\n",
    "\n",
    "# k iteration thread: checks k validity and adds to seeable next states in following timestep\n",
    "def k_working_thread(k_works, ts, state, states_can_be_visited, num_actions, current_qs, vi, var_bounds, state_gaps, state_vars, horizon,k, state_ms, transitions, rewards):\n",
    "    if states_can_be_visited[ts, state]:\n",
    "        # init vals\n",
    "        max_q = -np.inf\n",
    "        max_suboptimal_q = -np.inf\n",
    "        max_var = 0\n",
    "        # iterate over actions to get actual vals\n",
    "        for action in range(num_actions):\n",
    "            q = current_qs[ts, state, action]\n",
    "            max_q = max(max_q, q)\n",
    "            if vi.optimal_qs[ts,state,action] < vi.optimal_values[ts, state] - REWARD_PRECISION:\n",
    "                max_suboptimal_q = max(max_suboptimal_q, q)\n",
    "            max_var = max(max_var, var_bounds[ts, state, action])\n",
    "            # check for k fail condition\n",
    "            if max_q == max_suboptimal_q:\n",
    "                with lock:\n",
    "                    k_works = False\n",
    "            # otherwise get the state m value\n",
    "            else:\n",
    "                gap = max_q - max_suboptimal_q\n",
    "                state_gaps[ts, state] = gap\n",
    "                state_vars[ts, state] = max_var\n",
    "                m = math.ceil(16 * max_var / (gap**2) * math.log(2 * horizon * (num_actions**k)))\n",
    "                state_ms[ts, state] = max(1,m)\n",
    "            # iterate through actions to find next visitable states\n",
    "                for action in range(num_actions):\n",
    "                    if current_qs[ts, state, action] > max_suboptimal_q:\n",
    "                        next_state = transitions[state][action]\n",
    "                        # _, next_state, _, _ = env.P[state][action]\n",
    "                        if ts < horizon:\n",
    "                            states_can_be_visited[ts+1, next_state] = True\n",
    "\n",
    "\n",
    "# bellman backup thread: update the current qs and variance bounds for the run\n",
    "def bellman_backup_thread(ts, num_actions, transitions, rewards, state, current_qs, var_bounds):\n",
    "    for action in range(num_actions):\n",
    "        next_state = transitions[state][action]\n",
    "        reward = rewards[state][action]\n",
    "        #_, next_state, reward, _ = env.P[state][action]\n",
    "        max_next_q = -np.inf\n",
    "        max_next_var_bound = 0\n",
    "        for action in range(num_actions):\n",
    "            next_q = current_qs[ts+1, next_state, action]\n",
    "            max_next_q = max(max_next_q, next_q)\n",
    "            next_var_bound = var_bounds[ts+1, next_state, action]\n",
    "            max_next_var_bound = max(max_next_var_bound, next_var_bound)\n",
    "        current_qs[ts, state, action] = reward + max_next_q\n",
    "        var_bounds[ts, state, action] = max_next_var_bound\n",
    "\n",
    "\n",
    "# calculate EH bound using GORP bounds\n",
    "def get_EH_bound(transitions, rewards, horizon, exploration_policy=None):\n",
    "    num_states = len(transitions)#env.observation_space.n\n",
    "    num_actions = len(transitions[0])#env.action_space.n\n",
    "    # Perform value iteration\n",
    "    vi = value_iteration(transitions, rewards, horizon, exploration_policy)\n",
    "    print(\"done with value iteration\")\n",
    "    \n",
    "    # TODO updates not preserved in var_bounds object within the threads (need to fix)\n",
    "    # initialize and compute variance bounds\n",
    "    var_bounds = np.full((horizon, num_states, num_actions), 0)\n",
    "    for timestep in tqdm(range(horizon)):\n",
    "        # thread across different states in a given timestep, so no overwriting vals in data structures\n",
    "        threads = []\n",
    "        for state in vi.visitable_states[timestep]:\n",
    "            thread = threading.Thread(target=compute_variance_thread(var_bounds, timestep, state, num_actions, vi))\n",
    "            threads.append(thread)\n",
    "        # run the threads\n",
    "        for thread in threads:\n",
    "            thread.start()\n",
    "            sleep(0.005)\n",
    "        # merge the results of the threads\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "    print(\"done with variance bounds\")\n",
    "\n",
    "    # initialize q values, starting k, and EH results structure\n",
    "    current_qs = vi.exploration_qs\n",
    "    results = EHResults([],[],[],[],horizon)\n",
    "    k = 1\n",
    "\n",
    "    # find best working k\n",
    "    while k < results.effective_horizon:\n",
    "        # initialize results objects and iteration\n",
    "        k_works = True\n",
    "        state_ms = np.zeros((horizon, num_states))\n",
    "        state_vars = np.zeros((horizon, num_states))\n",
    "        state_gaps = np.zeros((horizon, num_states))\n",
    "        states_can_be_visited = np.full((horizon, num_states), False)\n",
    "        states_can_be_visited[0,0] = True\n",
    "        \n",
    "        for timestep in tqdm(range(horizon-1)):\n",
    "            # thread across different states in a given timestep, so no overwriting vals in data structures\n",
    "            threads = []\n",
    "            for state in vi.visitable_states[timestep]:\n",
    "                thread = threading.Thread(target=k_working_thread(k_works, timestep, state, states_can_be_visited, num_actions, current_qs, vi, var_bounds, state_gaps, state_vars, horizon, k, state_ms, transitions, rewards))\n",
    "                threads.append(thread)\n",
    "            # run the threads\n",
    "            for thread in threads:\n",
    "                thread.start()\n",
    "            # merge the results of the threads\n",
    "            for thread in threads:\n",
    "                thread.join()\n",
    "        if not k_works:\n",
    "            break\n",
    "        print(\"done with k check\")\n",
    "    \n",
    "        # if k works flag not triggered, then k is horizon and we update result vals accordingly\n",
    "        if k_works:\n",
    "            results.ks.append(k)\n",
    "            highest_m = np.max(state_ms)\n",
    "            timestep_state = np.argmax(state_ms)\n",
    "            # highest_m, timestep_state = findmax(state_ms)\n",
    "            print(timestep_state)\n",
    "            timestep, state = timestep_state\n",
    "            results.ms.append(highest_m)\n",
    "            # log of highest_m with base num_actions\n",
    "            H_k = k + math.log(highest_m, num_actions)\n",
    "            print(f\"H_{k} = {H_k}\")\n",
    "            results.gaps.append(state_gaps[timestep, state])\n",
    "            results.vars.append(state_vars[timestep, state])\n",
    "            results.effective_horizon = min(results.effective_horizon, H_k)\n",
    "        \n",
    "        # run a bellman backup\n",
    "        for timestep in tqdm(range(horizon-1)):\n",
    "            # thread across different states in a given timestep, so no overwriting vals in data structures\n",
    "                threads = []\n",
    "                for state in vi.visitable_states[timestep]:\n",
    "                    thread = threading.Thread(target=bellman_backup_thread(timestep, num_actions, transitions, rewards, state, current_qs, var_bounds))\n",
    "                    threads.append(thread)\n",
    "                # run the threads\n",
    "                for thread in threads:\n",
    "                    thread.start()\n",
    "                # merge the results of the threads\n",
    "                for thread in threads:\n",
    "                    thread.join()\n",
    "        print(\"done w bellman backup\")        \n",
    "        k += 1\n",
    "        print(\"k increased\")\n",
    "    print(\"EH results done\")\n",
    "    return results\n",
    "    \n",
    "\n",
    "# k_working_epw thread: checks if given state works for k\n",
    "def k_working_epw_thread(k_works, states_can_be_visited, ts, state, num_actions, current_qs, vi, transitions, rewards, horizon):\n",
    "    if states_can_be_visited[ts, state]:\n",
    "        # init and find max q\n",
    "        max_q = -np.inf\n",
    "        for action in range(num_actions):\n",
    "            max_q = max(max_q, current_qs[ts, state, action])\n",
    "        for action in range(num_actions):\n",
    "            # check if possible to take this action\n",
    "            if current_qs[ts, state, action] >= max_q:\n",
    "                if vi.optimal_qs[ts, state, action] < vi.optimal_values[ts,state] - REWARD_PRECISION:\n",
    "                    k_works = False\n",
    "                next_state = transitions[state][action]\n",
    "                #_, next_state, _, _ = env.P[state][action]\n",
    "                if ts < horizon:\n",
    "                    states_can_be_visited[ts+1, next_state] = True\n",
    "\n",
    "\n",
    "# epw_bellman_backup, modified bellman backup for epw\n",
    "def epw_bellman_backup_thread(ts, num_actions, state, transitions, rewards, current_qs):\n",
    "    for action in range(num_actions):\n",
    "        next_state = transitions[state][action]\n",
    "        reward = rewards[state][action]\n",
    "        #_, next_state, reward, _ = env.P[state][action]\n",
    "        max_next_q = -np.inf\n",
    "        for action in range(num_actions):\n",
    "            next_q = current_qs[ts+1, next_state, action]\n",
    "            max_next_q = max(max_next_q, next_q)\n",
    "        current_qs[ts, state, action] = reward + max_next_q\n",
    "\n",
    "\n",
    "# calculate EPW\n",
    "def get_EPW(transitions, rewards, horizon, exploration_policy=None, start_with_rewards=True):\n",
    "    # calculate min k for the gym environment and return it\n",
    "    num_states = len(transitions)#env.observation_space.n\n",
    "    num_actions = len(transitions[0])#env.action_space.n\n",
    "    # Perform value iteration\n",
    "    vi = value_iteration(transitions, rewards, horizon, exploration_policy)\n",
    "\n",
    "    # init current_qs\n",
    "    if start_with_rewards:\n",
    "        current_qs = np.full((horizon, num_states, num_actions), 0)\n",
    "        for timestep in tqdm(range(horizon)):\n",
    "            for state in vi.visitable_states[timestep]:\n",
    "                for action in range(num_actions):\n",
    "                    current_qs[timestep, state, action] = rewards[state][action]\n",
    "                    #_, _, current_qs[timestep, state, action], _ = env.P[state][action]\n",
    "    else:\n",
    "        current_qs = vi.exploration_qs\n",
    "\n",
    "    #\n",
    "    k = 1\n",
    "    while True:\n",
    "        # check if this k value works\n",
    "        k_works = True\n",
    "        states_can_be_visited = np.full((horizon, num_states), False)\n",
    "        states_can_be_visited[0,0] = True\n",
    "        for timestep in tqdm(range(horizon-1)):\n",
    "            # thread across different states in a given timestep, so no overwriting vals in data structures\n",
    "            threads = []\n",
    "            for state in vi.visitable_states[timestep]:\n",
    "                thread = threading.Thread(target=k_working_epw_thread(k_works, states_can_be_visited, timestep, state, num_actions, current_qs, vi, transitions, rewards, horizon))\n",
    "                threads.append(thread)\n",
    "            # run the threads\n",
    "            for thread in threads:\n",
    "                thread.start()\n",
    "            # merge the results of the threads\n",
    "            for thread in threads:\n",
    "                thread.join()\n",
    "        if not k_works:\n",
    "            break\n",
    "    \n",
    "        if k_works:\n",
    "            return k\n",
    "            \n",
    "        # otherwise run bellman backup and up k\n",
    "        for timestep in tqdm(range(horizon-1)):\n",
    "            # thread across different states in a given timestep, so no overwriting vals in data structures\n",
    "            threads = []\n",
    "            for state in vi.visitable_states[timestep]:\n",
    "                thread = threading.Thread(target=epw_bellman_backup_thread())\n",
    "                threads.append(thread)\n",
    "            # run the threads\n",
    "            for thread in threads:\n",
    "                thread.start()\n",
    "            # merge the results of the threads\n",
    "            for thread in threads:\n",
    "                thread.join()\n",
    "\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "## run through ##\n",
    "\n",
    "# load tables from dataset\n",
    "pong_table = np.load('/Users/laurenc/Documents/GitHub/282_expansion/data/bridge_dataset/mdps/pong_20_fs30/consolidated.npz')\n",
    "rewards = pong_table['rewards']\n",
    "transitions = pong_table['transitions']\n",
    "horizon =  len(transitions)\n",
    "\n",
    "c = 0\n",
    "for row in rewards:\n",
    "    for item in row:\n",
    "        if item > 0:\n",
    "            print(item)\n",
    "            c+=1\n",
    "print(c)\n",
    "\n",
    "# # calculate eh\n",
    "# eh_results = get_EH_bound(transitions, rewards, horizon)\n",
    "\n",
    "# # calculate EPW\n",
    "# epw_result = get_EPW(transitions, rewards, horizon)\n",
    "\n",
    "# # print results\n",
    "# print(f\"effective horizon: {eh_results.effective_horizon} \\n effective planning window: {epw_result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
